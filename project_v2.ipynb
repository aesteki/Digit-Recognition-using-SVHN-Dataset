{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saman/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/saman/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/saman/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/saman/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/saman/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/saman/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.feature import hog\n",
    "from skimage.color import rgb2grey\n",
    "import cv2 as cv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "matTr = loadmat('train_32x32.mat')\n",
    "matTe = loadmat('test_32x32.mat')\n",
    "# matExt = loadmat('extra_32x32.mat')\n",
    "\n",
    "# labels are originally in [1,10] and now will be in [0,9]\n",
    "Xtr, Ytr = matTr['X'], matTr['y']-1\n",
    "Xte, Yte = matTe['X'], matTe['y']-1\n",
    "# Xext, Yext = matExt['X'], matExt['y']-1\n",
    "\n",
    "# changing the dimensions so that the number of the input image is the first\n",
    "Xtr = np.transpose(Xtr, (3, 0, 1, 2))\n",
    "Xte = np.transpose(Xte, (3, 0, 1, 2))\n",
    "# Xext = np.transpose(Xext, (3, 0, 1, 2))\n",
    "\n",
    "# Xtr_ext = np.concatenate((Xtr,Xext))\n",
    "# Ytr_ext = np.concatenate((Ytr,Yext))\n",
    "Xtr, Xte = Xtr / 255.0, Xte / 255.0\n",
    "# Xtr_ext = Xtr_ext / 255.0\n",
    "Ytr = np.squeeze(Ytr)\n",
    "Yte = np.squeeze(Yte)\n",
    "# Yext = np.squeeze(Yext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "IMG_HEIGHT = 32\n",
    "IMG_WIDTH = 32\n",
    "NUM_CHANNEL = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtr_gray = np.zeros((len(Xtr),32,32,1))\n",
    "# for i in range(len(Xtr)):\n",
    "#     Xtr_gray[i] = tf.image.rgb_to_grayscale(Xtr[i])\n",
    "Xtr_gray = tf.image.rgb_to_grayscale(Xtr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_gray = K.eval(Xtr_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_bin = np.zeros((len(Xtr_gray),32,32,1))\n",
    "for i in range(len(Xtr_gray)):\n",
    "    Xtr_bin[i,:,:,0] = cv.adaptiveThreshold(Xtr_gray[i,:,:,0],255,cv.ADAPTIVE_THRESH_GAUSSIAN_C,cv.THRESH_BINARY,11,2)\n",
    "#     blur = cv.GaussianBlur(Xtr_gray[i,:,:,0],(5,5),0)\n",
    "#     _, Xtr_bin[i,:,:,0] = cv.threshold(blur,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "ind = 700\n",
    "titles = ['RGB Image', 'GRAY_SCALE', 'BINARY']\n",
    "images = [Xtr[ind], Xtr_gray[ind,:,:,0], Xtr_bin[ind,:,:,0]]\n",
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    if i!=0:\n",
    "        plt.imshow(images[i],'gray')\n",
    "    else:\n",
    "        plt.imshow(images[i])\n",
    "    plt.title(titles[i])\n",
    "    plt.xticks([]),plt.yticks([])\n",
    "plt.savefig('images.png',bbox_inches = 'tight')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "plt.hist(Ytr, ec='k',bins=10)\n",
    "plt.title('Histogram of Train Data')\n",
    "plt.savefig('histogram.png',bbox_inches = 'tight')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 65931 samples, validate on 7326 samples\n",
      "Epoch 1/20\n",
      " - 4s - loss: 1.0894 - acc: 0.6613 - val_loss: 0.7957 - val_acc: 0.7696\n",
      "Epoch 2/20\n",
      " - 2s - loss: 0.6454 - acc: 0.8130 - val_loss: 0.6697 - val_acc: 0.8079\n",
      "Epoch 3/20\n",
      " - 2s - loss: 0.5414 - acc: 0.8423 - val_loss: 0.5696 - val_acc: 0.8407\n",
      "Epoch 4/20\n",
      " - 2s - loss: 0.4770 - acc: 0.8622 - val_loss: 0.5427 - val_acc: 0.8430\n",
      "Epoch 5/20\n",
      " - 2s - loss: 0.4316 - acc: 0.8742 - val_loss: 0.5231 - val_acc: 0.8537\n",
      "Epoch 6/20\n",
      " - 2s - loss: 0.4035 - acc: 0.8814 - val_loss: 0.5243 - val_acc: 0.8542\n",
      "Epoch 7/20\n",
      " - 2s - loss: 0.3722 - acc: 0.8889 - val_loss: 0.5201 - val_acc: 0.8557\n",
      "Epoch 8/20\n",
      " - 2s - loss: 0.3507 - acc: 0.8958 - val_loss: 0.5108 - val_acc: 0.8557\n",
      "Epoch 9/20\n",
      " - 2s - loss: 0.3270 - acc: 0.9029 - val_loss: 0.5066 - val_acc: 0.8662\n",
      "Epoch 10/20\n",
      " - 2s - loss: 0.3052 - acc: 0.9089 - val_loss: 0.5110 - val_acc: 0.8609\n",
      "Epoch 11/20\n",
      " - 2s - loss: 0.2905 - acc: 0.9121 - val_loss: 0.5409 - val_acc: 0.8544\n",
      "Epoch 12/20\n",
      " - 2s - loss: 0.2768 - acc: 0.9179 - val_loss: 0.5635 - val_acc: 0.8552\n",
      "Epoch 13/20\n",
      " - 2s - loss: 0.2580 - acc: 0.9225 - val_loss: 0.5433 - val_acc: 0.8615\n",
      "Epoch 14/20\n",
      " - 2s - loss: 0.2446 - acc: 0.9261 - val_loss: 0.5298 - val_acc: 0.8627\n",
      "Epoch 15/20\n",
      " - 2s - loss: 0.2299 - acc: 0.9310 - val_loss: 0.5606 - val_acc: 0.8646\n",
      "Epoch 16/20\n",
      " - 2s - loss: 0.2233 - acc: 0.9329 - val_loss: 0.5648 - val_acc: 0.8643\n",
      "Epoch 17/20\n",
      " - 2s - loss: 0.2061 - acc: 0.9375 - val_loss: 0.6041 - val_acc: 0.8523\n",
      "Epoch 18/20\n",
      " - 2s - loss: 0.1978 - acc: 0.9400 - val_loss: 0.5895 - val_acc: 0.8601\n",
      "Epoch 19/20\n",
      " - 2s - loss: 0.1867 - acc: 0.9429 - val_loss: 0.5975 - val_acc: 0.8584\n",
      "Epoch 20/20\n",
      " - 2s - loss: 0.1808 - acc: 0.9439 - val_loss: 0.6257 - val_acc: 0.8605\n",
      "CNN 1: Epochs=15, Train accuracy=0.94387, Validation accuracy=0.86623\n",
      "Train on 65931 samples, validate on 7326 samples\n",
      "Epoch 1/20\n",
      " - 3s - loss: 0.8161 - acc: 0.7500 - val_loss: 0.5122 - val_acc: 0.8563\n",
      "Epoch 2/20\n",
      " - 3s - loss: 0.4295 - acc: 0.8783 - val_loss: 0.4838 - val_acc: 0.8568\n",
      "Epoch 3/20\n",
      " - 3s - loss: 0.3515 - acc: 0.8988 - val_loss: 0.3820 - val_acc: 0.8896\n",
      "Epoch 4/20\n",
      " - 3s - loss: 0.2990 - acc: 0.9128 - val_loss: 0.3760 - val_acc: 0.8923\n",
      "Epoch 5/20\n",
      " - 3s - loss: 0.2548 - acc: 0.9250 - val_loss: 0.3668 - val_acc: 0.8974\n",
      "Epoch 6/20\n",
      " - 3s - loss: 0.2171 - acc: 0.9374 - val_loss: 0.3749 - val_acc: 0.8969\n",
      "Epoch 7/20\n",
      " - 3s - loss: 0.1851 - acc: 0.9464 - val_loss: 0.4250 - val_acc: 0.8965\n",
      "Epoch 8/20\n",
      " - 3s - loss: 0.1568 - acc: 0.9532 - val_loss: 0.4262 - val_acc: 0.8911\n",
      "Epoch 9/20\n",
      " - 3s - loss: 0.1311 - acc: 0.9615 - val_loss: 0.5020 - val_acc: 0.8933\n",
      "Epoch 10/20\n",
      " - 3s - loss: 0.1129 - acc: 0.9654 - val_loss: 0.4724 - val_acc: 0.8897\n",
      "Epoch 11/20\n",
      " - 3s - loss: 0.0956 - acc: 0.9717 - val_loss: 0.4796 - val_acc: 0.8937\n",
      "Epoch 12/20\n",
      " - 3s - loss: 0.0849 - acc: 0.9752 - val_loss: 0.5068 - val_acc: 0.8911\n",
      "Epoch 13/20\n",
      " - 3s - loss: 0.0750 - acc: 0.9776 - val_loss: 0.5539 - val_acc: 0.8949\n",
      "Epoch 14/20\n",
      " - 3s - loss: 0.0642 - acc: 0.9817 - val_loss: 0.5763 - val_acc: 0.8931\n",
      "Epoch 15/20\n",
      " - 3s - loss: 0.0584 - acc: 0.9830 - val_loss: 0.6419 - val_acc: 0.8818\n",
      "Epoch 16/20\n",
      " - 3s - loss: 0.0575 - acc: 0.9833 - val_loss: 0.6354 - val_acc: 0.8938\n",
      "Epoch 17/20\n",
      " - 3s - loss: 0.0501 - acc: 0.9851 - val_loss: 0.6786 - val_acc: 0.8912\n",
      "Epoch 18/20\n",
      " - 3s - loss: 0.0518 - acc: 0.9847 - val_loss: 0.6869 - val_acc: 0.8974\n",
      "Epoch 19/20\n",
      " - 3s - loss: 0.0508 - acc: 0.9853 - val_loss: 0.7206 - val_acc: 0.8847\n",
      "Epoch 20/20\n",
      " - 3s - loss: 0.0419 - acc: 0.9887 - val_loss: 0.7266 - val_acc: 0.8934\n",
      "CNN 2: Epochs=15, Train accuracy=0.98873, Validation accuracy=0.89735\n",
      "Train on 65931 samples, validate on 7326 samples\n",
      "Epoch 1/20\n",
      " - 3s - loss: 0.9523 - acc: 0.6896 - val_loss: 0.5120 - val_acc: 0.8501\n",
      "Epoch 2/20\n",
      " - 3s - loss: 0.4005 - acc: 0.8813 - val_loss: 0.4207 - val_acc: 0.8761\n",
      "Epoch 3/20\n",
      " - 3s - loss: 0.3173 - acc: 0.9068 - val_loss: 0.3732 - val_acc: 0.8903\n",
      "Epoch 4/20\n",
      " - 3s - loss: 0.2704 - acc: 0.9199 - val_loss: 0.3654 - val_acc: 0.8972\n",
      "Epoch 5/20\n",
      " - 3s - loss: 0.2344 - acc: 0.9299 - val_loss: 0.3621 - val_acc: 0.9012\n",
      "Epoch 6/20\n",
      " - 3s - loss: 0.2039 - acc: 0.9394 - val_loss: 0.3580 - val_acc: 0.9016\n",
      "Epoch 7/20\n",
      " - 3s - loss: 0.1761 - acc: 0.9475 - val_loss: 0.3894 - val_acc: 0.9002\n",
      "Epoch 8/20\n",
      " - 3s - loss: 0.1535 - acc: 0.9542 - val_loss: 0.3749 - val_acc: 0.9008\n",
      "Epoch 9/20\n",
      " - 3s - loss: 0.1341 - acc: 0.9604 - val_loss: 0.4101 - val_acc: 0.9031\n",
      "Epoch 10/20\n",
      " - 3s - loss: 0.1143 - acc: 0.9646 - val_loss: 0.4401 - val_acc: 0.8997\n",
      "Epoch 11/20\n",
      " - 3s - loss: 0.1022 - acc: 0.9686 - val_loss: 0.4758 - val_acc: 0.9038\n",
      "Epoch 12/20\n",
      " - 3s - loss: 0.0923 - acc: 0.9719 - val_loss: 0.4521 - val_acc: 0.9049\n",
      "Epoch 13/20\n",
      " - 3s - loss: 0.0868 - acc: 0.9727 - val_loss: 0.5209 - val_acc: 0.8997\n",
      "Epoch 14/20\n",
      " - 3s - loss: 0.0836 - acc: 0.9745 - val_loss: 0.5591 - val_acc: 0.8908\n",
      "Epoch 15/20\n",
      " - 3s - loss: 0.0622 - acc: 0.9808 - val_loss: 0.5912 - val_acc: 0.8953\n",
      "Epoch 16/20\n",
      " - 3s - loss: 0.0752 - acc: 0.9773 - val_loss: 0.6308 - val_acc: 0.8920\n",
      "Epoch 17/20\n",
      " - 3s - loss: 0.0638 - acc: 0.9808 - val_loss: 0.6012 - val_acc: 0.8999\n",
      "Epoch 18/20\n",
      " - 3s - loss: 0.0588 - acc: 0.9824 - val_loss: 0.6398 - val_acc: 0.8930\n",
      "Epoch 19/20\n",
      " - 3s - loss: 0.0654 - acc: 0.9808 - val_loss: 0.6153 - val_acc: 0.8999\n",
      "Epoch 20/20\n",
      " - 3s - loss: 0.0598 - acc: 0.9825 - val_loss: 0.6226 - val_acc: 0.8908\n",
      "CNN 3: Epochs=15, Train accuracy=0.98251, Validation accuracy=0.90486\n"
     ]
    }
   ],
   "source": [
    "nets = 3\n",
    "model = [0]*nets\n",
    "history = [0]*nets\n",
    "for i in range(nets):\n",
    "    model[i] = Sequential()\n",
    "    model[i].add(Conv2D(16, kernel_size=5, activation='relu', padding=\"same\", input_shape=(32,32,3)))\n",
    "    model[i].add(MaxPooling2D())\n",
    "    if i>0:\n",
    "        model[i].add(Conv2D(32, kernel_size=5, activation='relu', padding=\"same\"))\n",
    "        model[i].add(MaxPooling2D())\n",
    "    if i>1:\n",
    "        model[i].add(Conv2D(64, kernel_size=5, activation='relu', padding=\"same\"))\n",
    "        model[i].add(MaxPooling2D(padding='same'))\n",
    "    model[i].add(Flatten())\n",
    "    model[i].add(Dense(256, activation='relu'))\n",
    "    model[i].add(Dense(10, activation='softmax'))\n",
    "    model[i].compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(Xtr, Ytr, test_size = 0.1)\n",
    "for j in range(nets):\n",
    "    history[j] = model[j].fit(X_train2, Y_train2, epochs=20, batch_size=64,\n",
    "                              validation_data = (X_val2, Y_val2), verbose=2)\n",
    "    print(\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
    "            j+1,epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e911a7e5383e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     print(\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n\u001b[0;32m----> 4\u001b[0;31m             j+1,epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "epochs=20\n",
    "for j in range(3):\n",
    "    print(\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
    "            j+1,epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nets):\n",
    "    val_acc = history[i].history['val_accuracy']\n",
    "    epochs_range = range(epochs)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(epochs_range, val_acc, label='(C-P)x{}'.format(i+1))\n",
    "    \n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = 6\n",
    "model = [0]*nets\n",
    "history = [0]*nets\n",
    "for i in range(nets):\n",
    "    model[i] = Sequential()\n",
    "    model[i].add(Conv2D(i*8+8, kernel_size=5, activation='relu', padding=\"same\", input_shape=(32,32,3)))\n",
    "    model[i].add(MaxPooling2D())\n",
    "    model[i].add(Conv2D(i*16+16, kernel_size=5, activation='relu', padding=\"same\"))\n",
    "    model[i].add(MaxPooling2D())\n",
    "    model[i].add(Flatten())\n",
    "    model[i].add(Dense(256, activation='relu'))\n",
    "    model[i].add(Dense(10, activation='softmax'))\n",
    "    model[i].compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(Xtr, Ytr, test_size = 0.1)\n",
    "for j in range(nets):\n",
    "    history[j] = model[j].fit(X_train2, Y_train2, epochs=20, batch_size=64,\n",
    "                              validation_data = (X_val2, Y_val2), verbose=2)\n",
    "    print(\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
    "            j+1,epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20\n",
    "for j in range(nets):\n",
    "    print(\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
    "            j+1,epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nets):\n",
    "    val_acc = history[i].history['val_accuracy']\n",
    "    epochs_range = range(epochs)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(epochs_range, val_acc, label='{} maps'.format(i*8+8))\n",
    "    \n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = 8\n",
    "model = [0]*nets\n",
    "history = [0]*nets\n",
    "for i in range(nets):\n",
    "    model[i] = Sequential()\n",
    "    model[i].add(Conv2D(32, kernel_size=5, activation='relu', padding=\"same\", input_shape=(32,32,3)))\n",
    "    model[i].add(MaxPooling2D())\n",
    "    model[i].add(Conv2D(64, kernel_size=5, activation='relu', padding=\"same\"))\n",
    "    model[i].add(MaxPooling2D())\n",
    "    model[i].add(Flatten())\n",
    "    if i>0:\n",
    "        model[i].add(Dense(2**(i+4), activation='relu'))\n",
    "    model[i].add(Dense(10, activation='softmax'))\n",
    "    model[i].compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(Xtr, Ytr, test_size = 0.1)\n",
    "for j in range(nets):\n",
    "    history[j] = model[j].fit(X_train2, Y_train2, epochs=20, batch_size=64,\n",
    "                              validation_data = (X_val2, Y_val2), verbose=2)\n",
    "    print(\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
    "            j+1,epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20\n",
    "for j in range(nets):\n",
    "    print(\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
    "            j+1,epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nets):\n",
    "    val_acc = history[i].history['val_accuracy']\n",
    "    epochs_range = range(epochs)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(epochs_range, val_acc, label='{}N'.format(2**(i+4)))\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = 8\n",
    "model = [0]*nets\n",
    "history = [0]*nets\n",
    "for i in range(nets):\n",
    "    model[i] = Sequential()\n",
    "    model[i].add(Conv2D(32, kernel_size=5, activation='relu', padding=\"same\", input_shape=(32,32,3)))\n",
    "    model[i].add(MaxPooling2D())\n",
    "    model[i].add(Dropout(i*0.1))\n",
    "    model[i].add(Conv2D(64, kernel_size=5, activation='relu', padding=\"same\"))\n",
    "    model[i].add(MaxPooling2D())\n",
    "    model[i].add(Dropout(i*0.1))\n",
    "    model[i].add(Flatten())\n",
    "    model[i].add(Dense(2**(i+4), activation='relu'))\n",
    "    model[i].add(Dropout(i*0.1))\n",
    "    model[i].add(Dense(10, activation='softmax'))\n",
    "    model[i].compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(Xtr, Ytr, test_size = 0.1)\n",
    "for j in range(nets):\n",
    "    history[j] = model[j].fit(X_train2, Y_train2, epochs=20, batch_size=64,\n",
    "                              validation_data = (X_val2, Y_val2), verbose=2)\n",
    "    print(\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
    "            j+1,epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20\n",
    "for j in range(nets):\n",
    "    print(\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
    "            j+1,epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nets):\n",
    "    val_acc = history[i].history['val_accuracy']\n",
    "    epochs_range = range(epochs)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(epochs_range, val_acc, label='D={}'.format(i*0.1))\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(32,32,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, 3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model[i].add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "epochs = 20\n",
    "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(Xtr, Ytr, test_size = 0.1)\n",
    "history = model.fit(X_train2, Y_train2, epochs=20, batch_size=64,\n",
    "                              validation_data = (X_val2, Y_val2), verbose=2)\n",
    "print(\"CNN: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
    "            epochs,max(history.history['acc']),max(history.history['val_acc']) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CREATE MORE IMAGES VIA DATA AUGMENTATION\n",
    "# datagen = ImageDataGenerator(\n",
    "#         rotation_range=10,  \n",
    "#         zoom_range = 0.10,  \n",
    "#         width_shift_range=0.1, \n",
    "#         height_shift_range=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = 5\n",
    "model = [0]*nets\n",
    "history = [0]*nets\n",
    "for i in range(3):\n",
    "    model[i] = Sequential()\n",
    "    model[i].add(Conv2D(16, kernel_size=5, activation='relu', padding=\"same\", input_shape=(32,32,3)))\n",
    "    model[i].add(MaxPooling2D(pool_size=(2,2)))\n",
    "    if i>0:\n",
    "        model[i].add(Conv2D(32, kernel_size=5, activation='relu', padding=\"same\"))\n",
    "        model[i].add(MaxPooling2D(pool_size=(2,2)))\n",
    "    if i>1:\n",
    "        model[i].add(Conv2D(64, kernel_size=5, activation='relu', padding=\"same\"))\n",
    "        model[i].add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "    model[i].add(Flatten())\n",
    "    model[i].add(Dense(256, activation='relu'))\n",
    "    model[i].add(Dense(10, activation='softmax'))\n",
    "    myAdam = tf.keras.optimizers.Adam(lr=0.00001)\n",
    "    model[i].compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(Xtr, Ytr, test_size = 0.1)\n",
    "for j in range(3):\n",
    "    history[j] = model[j].fit(X_train2, Y_train2, epochs=20, batch_size=64,\n",
    "                              validation_data = (X_val2, Y_val2), verbose=2)\n",
    "    print(\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
    "            j+1,epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for j in range(nets):\n",
    "#     X_train2, X_val2, Y_train2, Y_val2 = train_test_split(Xtr, Ytr, test_size = 0.1)\n",
    "#     history[j] = model[j].fit_generator(datagen.flow(X_train2,Y_train2, batch_size=64),\n",
    "#         epochs = epochs, steps_per_epoch = X_train2.shape[0]//64,  \n",
    "#         validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=0)\n",
    "#     print(\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
    "#         j+1,epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))\n",
    "epochs = 20\n",
    "# DECREASE LEARNING RATE EACH EPOCH\n",
    "# annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n",
    "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(Xtr, Ytr, test_size = 0.1)\n",
    "for j in range(3):\n",
    "    history[j] = model[j].fit(X_train2, Y_train2, epochs=20, batch_size=64,\n",
    "                              validation_data = (X_val2, Y_val2), verbose=2)\n",
    "    print(\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
    "            j+1,epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # DECREASE LEARNING RATE EACH EPOCH\n",
    "# annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n",
    "\n",
    "\n",
    "# epochs=50\n",
    "# X_train2, X_val2, Y_train2, Y_val2 = train_test_split(Xtr, Ytr, test_size = 0.1)\n",
    "# history = model.fit_generator(datagen.flow(X_train2,Y_train2, batch_size=64),\n",
    "#                               epochs = epochs, steps_per_epoch = X_train2.shape[0]//64,  \n",
    "#                               validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=0)\n",
    "# print(\"CNN: Epochs={0:d}, Train accuracy={1:.5f}, Validation accuracy={2:.5f}\".format(\n",
    "#     epochs,history['acc'],history['val_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(img):\n",
    "    # flatten three channel color image\n",
    "    color_features = img.flatten()\n",
    "    # convert image to greyscale\n",
    "    grey_image = rgb2grey(img)\n",
    "    # get HOG features from greyscale image\n",
    "    hog_features, _ = hog(img, orientations=8, pixels_per_cell=(16, 16),\n",
    "                    cells_per_block=(1, 1), visualize=True, multichannel=True)\n",
    "    # combine color and hog features into a single array\n",
    "    flat_features = np.hstack((color_features,hog_features))\n",
    "    return hog_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = []\n",
    "for i in range(len(Xtr)):\n",
    "    image_features = create_features(Xtr[i])\n",
    "    features_list.append(image_features)  \n",
    "# convert list of arrays into a matrix\n",
    "feature_matrix = np.array(features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the features\n",
    "from sklearn.preprocessing import scale\n",
    "X_scaled = scale(feature_matrix)\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, Ytr, test_size = 0.3, train_size = 0.2 ,random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_linear = SVC(kernel='linear')\n",
    "model_linear.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = model_linear.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix and accuracy\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# accuracy\n",
    "print(\"accuracy:\", metrics.accuracy_score(y_true=y_test, y_pred=y_pred), \"\\n\")\n",
    "\n",
    "# cm\n",
    "print(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
