{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.feature import hog\n",
    "from skimage.color import rgb2grey\n",
    "import cv2 as cv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The version_information extension is already loaded. To reload it, use:\n",
      "  %reload_ext version_information\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "Software versions": [
        {
         "module": "Python",
         "version": "3.7.4 64bit [Clang 4.0.1 (tags/RELEASE_401/final)]"
        },
        {
         "module": "IPython",
         "version": "7.12.0"
        },
        {
         "module": "OS",
         "version": "Darwin 19.3.0 x86_64 i386 64bit"
        },
        {
         "module": "jupyter",
         "version": "The 'jupyter' distribution was not found and is required by the application"
        },
        {
         "module": "pandas",
         "version": "1.0.0"
        },
        {
         "module": "matplotlib",
         "version": "3.1.3"
        },
        {
         "module": "sklearn",
         "version": "0.21.2"
        },
        {
         "module": "scipy",
         "version": "1.4.1"
        },
        {
         "module": "tensorflow",
         "version": "1.15.0"
        },
        {
         "module": "cv2",
         "version": "4.1.0"
        }
       ]
      },
      "text/html": [
       "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.7.4 64bit [Clang 4.0.1 (tags/RELEASE_401/final)]</td></tr><tr><td>IPython</td><td>7.12.0</td></tr><tr><td>OS</td><td>Darwin 19.3.0 x86_64 i386 64bit</td></tr><tr><td>jupyter</td><td>The 'jupyter' distribution was not found and is required by the application</td></tr><tr><td>pandas</td><td>1.0.0</td></tr><tr><td>matplotlib</td><td>3.1.3</td></tr><tr><td>sklearn</td><td>0.21.2</td></tr><tr><td>scipy</td><td>1.4.1</td></tr><tr><td>tensorflow</td><td>1.15.0</td></tr><tr><td>cv2</td><td>4.1.0</td></tr><tr><td colspan='2'>Thu Mar 12 20:39:40 2020 PDT</td></tr></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{|l|l|}\\hline\n",
       "{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\n",
       "Python & 3.7.4 64bit [Clang 4.0.1 (tags/RELEASE\\_401/final)] \\\\ \\hline\n",
       "IPython & 7.12.0 \\\\ \\hline\n",
       "OS & Darwin 19.3.0 x86\\_64 i386 64bit \\\\ \\hline\n",
       "jupyter & The 'jupyter' distribution was not found and is required by the application \\\\ \\hline\n",
       "pandas & 1.0.0 \\\\ \\hline\n",
       "matplotlib & 3.1.3 \\\\ \\hline\n",
       "sklearn & 0.21.2 \\\\ \\hline\n",
       "scipy & 1.4.1 \\\\ \\hline\n",
       "tensorflow & 1.15.0 \\\\ \\hline\n",
       "cv2 & 4.1.0 \\\\ \\hline\n",
       "\\hline \\multicolumn{2}{|l|}{Thu Mar 12 20:39:40 2020 PDT} \\\\ \\hline\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "Software versions\n",
       "Python 3.7.4 64bit [Clang 4.0.1 (tags/RELEASE_401/final)]\n",
       "IPython 7.12.0\n",
       "OS Darwin 19.3.0 x86_64 i386 64bit\n",
       "jupyter The 'jupyter' distribution was not found and is required by the application\n",
       "pandas 1.0.0\n",
       "matplotlib 3.1.3\n",
       "sklearn 0.21.2\n",
       "scipy 1.4.1\n",
       "tensorflow 1.15.0\n",
       "cv2 4.1.0\n",
       "Thu Mar 12 20:39:40 2020 PDT"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by github/jrjohansson\n",
    "# ! pip install --user version_information\n",
    "%load_ext version_information\n",
    "%version_information pandas, matplotlib, sklearn, scipy, tensorflow, cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "matTr = loadmat('train_32x32.mat')\n",
    "matTe = loadmat('test_32x32.mat')\n",
    "# matExt = loadmat('extra_32x32.mat')\n",
    "\n",
    "# labels are originally in [1,10] and now will be in [0,9]\n",
    "Xtr, Ytr = matTr['X'], matTr['y']-1\n",
    "Xte, Yte = matTe['X'], matTe['y']-1\n",
    "# Xext, Yext = matExt['X'], matExt['y']-1\n",
    "\n",
    "# changing the dimensions so that the number of the input image is the first\n",
    "Xtr = np.transpose(Xtr, (3, 0, 1, 2))\n",
    "Xte = np.transpose(Xte, (3, 0, 1, 2))\n",
    "# Xext = np.transpose(Xext, (3, 0, 1, 2))\n",
    "\n",
    "# Xtr_ext = np.concatenate((Xtr,Xext))\n",
    "# Ytr_ext = np.concatenate((Ytr,Yext))\n",
    "\n",
    "# Xtr_ext = Xtr_ext / 255.0\n",
    "Ytr = np.squeeze(Ytr)\n",
    "Yte = np.squeeze(Yte)\n",
    "# Yext = np.squeeze(Yext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "IMG_HEIGHT = 32\n",
    "IMG_WIDTH = 32\n",
    "NUM_CHANNEL = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtr_gray = np.zeros((len(Xtr),32,32,1))\n",
    "# for i in range(len(Xtr)):\n",
    "#     Xtr_gray[i] = tf.image.rgb_to_grayscale(Xtr[i])\n",
    "Xtr_gray = tf.image.rgb_to_grayscale(Xtr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_gray = K.eval(Xtr_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_bin = np.zeros((len(Xtr_gray),32,32,1))\n",
    "for i in range(len(Xtr_gray)):\n",
    "    Xtr_bin[i,:,:,0] = cv.adaptiveThreshold(Xtr_gray[i,:,:,0],255,cv.ADAPTIVE_THRESH_GAUSSIAN_C,cv.THRESH_BINARY,11,2)\n",
    "#     blur = cv.GaussianBlur(Xtr_gray[i,:,:,0],(5,5),0)\n",
    "#     _, Xtr_bin[i,:,:,0] = cv.threshold(blur,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "ind = 700\n",
    "titles = ['RGB Image', 'GRAY_SCALE', 'BINARY']\n",
    "images = [Xtr[ind], Xtr_gray[ind,:,:,0], Xtr_bin[ind,:,:,0]]\n",
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    if i!=0:\n",
    "        plt.imshow(images[i],'gray')\n",
    "    else:\n",
    "        plt.imshow(images[i])\n",
    "    plt.title(titles[i])\n",
    "    plt.xticks([]),plt.yticks([])\n",
    "plt.savefig('images.png',bbox_inches = 'tight')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "plt.hist(Ytr, ec='k',bins=10)\n",
    "plt.title('Histogram of Train Data')\n",
    "plt.savefig('histogram.png',bbox_inches = 'tight')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(32,32,3)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(32, 3, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.4))\n",
    "\n",
    "# model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.4))\n",
    "\n",
    "# model.add(Conv2D(128, kernel_size = 4, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Flatten())\n",
    "# model.add(Dropout(0.4))\n",
    "# model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = 5\n",
    "model = [0]*nets\n",
    "history = [0]*nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    model[i] = Sequential()\n",
    "    model[i].add(Conv2D(24, kernel_size=5, activation='relu', padding=\"same\", input_shape=(32,32,1)))\n",
    "    model[i].add(MaxPooling2D(pool_size=(2,2)))\n",
    "    if i>0:\n",
    "        model[i].add(Conv2D(48, kernel_size=5, activation='relu', padding=\"same\"))\n",
    "        model[i].add(MaxPooling2D(pool_size=(2,2)))\n",
    "    if i>1:\n",
    "        model[i].add(Conv2D(64, kernel_initializer='he_normal', kernel_size=5, activation='relu', padding=\"same\"))\n",
    "        model[i].add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "    model[i].add(Flatten())\n",
    "    model[i].add(Dense(256, activation='relu'))\n",
    "    model[i].add(Dense(10, activation='softmax'))\n",
    "    myAdam = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "    model[i].compile(optimizer=myAdam,\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CREATE MORE IMAGES VIA DATA AUGMENTATION\n",
    "# datagen = ImageDataGenerator(\n",
    "#         rotation_range=10,  \n",
    "#         zoom_range = 0.10,  \n",
    "#         width_shift_range=0.1, \n",
    "#         height_shift_range=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 65931 samples, validate on 7326 samples\n",
      "Epoch 1/20\n",
      "65931/65931 - 35s - loss: 2.3696 - acc: 0.0914 - val_loss: 2.3700 - val_acc: 0.0912\n",
      "Epoch 2/20\n",
      "65931/65931 - 37s - loss: 2.3669 - acc: 0.0943 - val_loss: 2.3700 - val_acc: 0.0912\n",
      "Epoch 3/20\n",
      "65931/65931 - 35s - loss: 2.3669 - acc: 0.0943 - val_loss: 2.3700 - val_acc: 0.0912\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-342dd286afc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mX_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtr_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m history[j] = model[j].fit(X_train2, Y_train2, epochs=20, batch_size=64,\n\u001b[0;32m---> 14\u001b[0;31m                           validation_data = (X_val2, Y_val2), verbose=2)\n\u001b[0m\u001b[1;32m     15\u001b[0m print(\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n\u001b[1;32m     16\u001b[0m         j+1,epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for j in range(nets):\n",
    "#     X_train2, X_val2, Y_train2, Y_val2 = train_test_split(Xtr, Ytr, test_size = 0.1)\n",
    "#     history[j] = model[j].fit_generator(datagen.flow(X_train2,Y_train2, batch_size=64),\n",
    "#         epochs = epochs, steps_per_epoch = X_train2.shape[0]//64,  \n",
    "#         validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=0)\n",
    "#     print(\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
    "#         j+1,epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))\n",
    "j=0\n",
    "epochs = 20\n",
    "# DECREASE LEARNING RATE EACH EPOCH\n",
    "# annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n",
    "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(Xtr_bin, Ytr, test_size = 0.1)\n",
    "history[j] = model[j].fit(X_train2, Y_train2, epochs=20, batch_size=64,\n",
    "                          validation_data = (X_val2, Y_val2), verbose=2)\n",
    "print(\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
    "        j+1,epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # DECREASE LEARNING RATE EACH EPOCH\n",
    "# annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n",
    "\n",
    "\n",
    "# epochs=50\n",
    "# X_train2, X_val2, Y_train2, Y_val2 = train_test_split(Xtr, Ytr, test_size = 0.1)\n",
    "# history = model.fit_generator(datagen.flow(X_train2,Y_train2, batch_size=64),\n",
    "#                               epochs = epochs, steps_per_epoch = X_train2.shape[0]//64,  \n",
    "#                               validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=0)\n",
    "# print(\"CNN: Epochs={0:d}, Train accuracy={1:.5f}, Validation accuracy={2:.5f}\".format(\n",
    "#     epochs,history['acc'],history['val_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(img):\n",
    "    # flatten three channel color image\n",
    "    color_features = img.flatten()\n",
    "    # convert image to greyscale\n",
    "    grey_image = rgb2grey(img)\n",
    "    # get HOG features from greyscale image\n",
    "    hog_features, _ = hog(img, orientations=8, pixels_per_cell=(16, 16),\n",
    "                    cells_per_block=(1, 1), visualize=True, multichannel=True)\n",
    "    # combine color and hog features into a single array\n",
    "    flat_features = np.hstack((color_features,hog_features))\n",
    "    return hog_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = []\n",
    "for i in range(len(Xtr)):\n",
    "    image_features = create_features(Xtr[i])\n",
    "    features_list.append(image_features)  \n",
    "# convert list of arrays into a matrix\n",
    "feature_matrix = np.array(features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the features\n",
    "from sklearn.preprocessing import scale\n",
    "X_scaled = scale(feature_matrix)\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, Ytr, test_size = 0.3, train_size = 0.2 ,random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_linear = SVC(kernel='linear')\n",
    "model_linear.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = model_linear.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix and accuracy\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# accuracy\n",
    "print(\"accuracy:\", metrics.accuracy_score(y_true=y_test, y_pred=y_pred), \"\\n\")\n",
    "\n",
    "# cm\n",
    "print(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
